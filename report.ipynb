{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (2.2.2)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107520610>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107520970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107520b20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107520cd0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x107520eb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (1.26.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sentence-transformers) (0.19.0)\n",
      "Requirement already satisfied: filelock in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: click in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/remicarvalot/miniforge3/envs/llmGpu/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sentence transformers if you don't have\n",
    "#import libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/remicarvalot/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model  = SentenceTransformer(\"bert-base-uncased\")\n",
    "# see the details of model with print \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will report each results in a same dataframe to plot the different results in function of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['model_name','model_type','method_preprocessing','embedding_model','chunk_size','chunk_overlap','context_length','chunk_technique','temperature','gpu_layers','search_kwargs','size_model','size_answer','reorder','quantization','device','rate','rate_table','rate_text','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_report = pd.DataFrame(columns=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>method_preprocessing</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>context_length</th>\n",
       "      <th>chunk_technique</th>\n",
       "      <th>temperature</th>\n",
       "      <th>gpu_layers</th>\n",
       "      <th>search_kwargs</th>\n",
       "      <th>size_model</th>\n",
       "      <th>size_answer</th>\n",
       "      <th>reorder</th>\n",
       "      <th>quantization</th>\n",
       "      <th>device</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate_table</th>\n",
       "      <th>rate_text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, model_type, method_preprocessing, embedding_model, chunk_size, chunk_overlap, context_length, chunk_technique, temperature, gpu_layers, search_kwargs, size_model, size_answer, reorder, quantization, device, rate, rate_table, rate_text, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculationRate(csv_filename):\n",
    "    df = pd.read_csv(csv_filename,sep=';',encoding='utf-8')\n",
    "    # creating list of sentences\n",
    "    sentences_list_result= df['ResultLLM']\n",
    "    print('sentences_list_result : ', sentences_list_result)\n",
    "    sentences_list_baseline= df['Baseline']\n",
    "    #takin embedding of list\n",
    "    embeddings_result=  model.encode(sentences_list_result)\n",
    "    embeddings_baseline = model.encode(sentences_list_baseline)\n",
    "\n",
    "    rate = 0\n",
    "    rateText = 0\n",
    "    rateTable = 0\n",
    "    similarity_rate=0.7\n",
    "    list_questions_defaillantes = []\n",
    "    for row in range(1,len(embeddings_result)):\n",
    "        result = cosine_similarity(embeddings_result[row-1:row],embeddings_baseline[row-1:row]) \n",
    "        if result>similarity_rate:\n",
    "            rate=rate+1\n",
    "            if row>=(len(embeddings_result)-6):\n",
    "                rateTable = rateTable+1\n",
    "            else:\n",
    "                rateText = rateText+1\n",
    "        else:\n",
    "            list_questions_defaillantes.append([sentences_list_result[row],sentences_list_baseline[row]])\n",
    "        '''print('len(embeddings_result)-6 ',len(embeddings_result)-6)\n",
    "        print('len(embeddings_result) ',len(embeddings_result))\n",
    "        print('rateTable : ',rateTable)\n",
    "        print('rateText : ',rateText)\n",
    "        print('rate : ',rate)'''\n",
    "    return rate/len(embeddings_result), rateText/(len(embeddings_result)-6), rateTable/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_static_paramaters(csv_file_parameters,dataframe_report):\n",
    "    df = pd.read_csv(csv_file_parameters, delimiter=',')\n",
    "    #print('size df.columns : ', len(df.columns))\n",
    "    #print('size dataframe_report.columns : ', len(dataframe_report.columns))\n",
    "    #print(dataframe_report)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        #print('col = ', col)\n",
    "        #print(df[col].values)\n",
    "        dataframe_report[col]=df[col].values\n",
    "    return dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_rates(csv_file,dataframe_report):\n",
    "    rate,rateText,rateTable=calculationRate(csv_file)\n",
    "    dataframe_report['rate']=rate\n",
    "    dataframe_report['rate_table']=rateTable\n",
    "    dataframe_report['rate_text']=rateText\n",
    "    return dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_parameters = './results/parameterllama-2-13b-chat.Q4_K_M.gguf75690919-de1b-43ba-a4e1-ccd3cd45592c.csv'\n",
    "csv_file = './results/llama-2-13b-chat.Q4_K_M.gguf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_out_static_paramaters(csv_file_parameters,dataframe_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>method_preprocessing</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>context_length</th>\n",
       "      <th>size_model</th>\n",
       "      <th>size_answer</th>\n",
       "      <th>reorder</th>\n",
       "      <th>quantization</th>\n",
       "      <th>device</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate_table</th>\n",
       "      <th>rate_text</th>\n",
       "      <th>chunk_technique</th>\n",
       "      <th>temperature</th>\n",
       "      <th>gpu_layers</th>\n",
       "      <th>search_kwargs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-2-13b-chat.Q4_K_M.gguf</td>\n",
       "      <td>llama</td>\n",
       "      <td>PaddleOCR</td>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>7B</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>GGUF</td>\n",
       "      <td>MPS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RecursiveTextSplitter</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name model_type method_preprocessing  \\\n",
       "0  llama-2-13b-chat.Q4_K_M.gguf      llama            PaddleOCR   \n",
       "\n",
       "          embedding_model  chunk_size  chunk_overlap  context_length  \\\n",
       "0  BAAI/bge-large-en-v1.5         200             64            2048   \n",
       "\n",
       "  size_model  size_answer  reorder quantization device  rate  rate_table  \\\n",
       "0         7B          200     True         GGUF    MPS     0           0   \n",
       "\n",
       "   rate_text        chunk_technique  temperature  gpu_layers  search_kwargs  \n",
       "0          0  RecursiveTextSplitter          0.1          20              5  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_report = fill_out_rates(csv_file,dataframe_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>method_preprocessing</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>context_length</th>\n",
       "      <th>size_model</th>\n",
       "      <th>size_answer</th>\n",
       "      <th>reorder</th>\n",
       "      <th>quantization</th>\n",
       "      <th>device</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate_table</th>\n",
       "      <th>rate_text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, model_type, method_preprocessing, embedding_model, chunk_size, chunk_overlap, context_length, size_model, size_answer, reorder, quantization, device, rate, rate_table, rate_text, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = './results'\n",
    "path_parameters = './parameters'\n",
    "def create_df_results(path_results,path_parameters):\n",
    "    dataframe_report = pd.DataFrame(columns=parameters)\n",
    "    row=0\n",
    "    for filename in os.listdir(path_results):\n",
    "        print(filename)\n",
    "        dataframe_report.loc[row, :] = parameters\n",
    "        dataframe_report.iloc[row] = fill_out_static_paramaters(path_parameters+'/parameter'+filename,dataframe_report.iloc[row])\n",
    "        dataframe_report.iloc[row] = fill_out_rates(path_results+'/'+filename,dataframe_report.iloc[row])\n",
    "        row = row +1\n",
    "    return dataframe_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openchat_3.5.Q4_K_M.gguf8a3d19e3-332e-4a06-80bb-52df59a801de.csv\n",
      "sentences_list_result :  0      BART is a model that uses concatenated questi...\n",
      "1      Some advantages of BART's setup include its s...\n",
      "2      BART handles fine-tuning for machine translat...\n",
      "3      The text does not provide specific informatio...\n",
      "4      BART performs comparably to other pretraining...\n",
      "5      BART's architecture differs from BERT and GPT...\n",
      "6      The key advantage of BART's setup is that it ...\n",
      "7      The purpose of the in-filling scheme in BART ...\n",
      "8      BART performs well in fine-tuned text generat...\n",
      "9      BART brings a 1.1 BLEU increase over a back-t...\n",
      "10     The text does not provide specific pre-traini...\n",
      "11     BART uses token masking in its pre-training o...\n",
      "12     The text does not provide specific informatio...\n",
      "13     The significance of token masking in BART's p...\n",
      "14     BART performs similarly to other models on SQ...\n",
      "15     The key trends observed in the results of pre...\n",
      "16     The experimental setup for BART's large-scale...\n",
      "17     BART performs significantly better on summari...\n",
      "18     The text does not provide specific results fo...\n",
      "19     The F1 score of BART on the SQuAD 1.1 task is...\n",
      "20     I don't know the accuracy of BART on the MNLI...\n",
      "21     The text does not provide information about t...\n",
      "22     I don't know the ROUGE scores for BART compar...\n",
      "23     BART shows large improvements on summarizatio...\n",
      "24     BART achieves an EM/F1 score of 86.1/89.2 on ...\n",
      "Name: ResultLLM, dtype: object\n",
      "openchat_3.5.Q4_K_M.ggufc3dd60fe-c5dc-458c-b775-dc638c8238ed.csv\n",
      "sentences_list_result :  0      BART is a model that stands for \"Bidirectiona...\n",
      "1      Some advantages of BART's setup include its s...\n",
      "2      BART handles fine-tuning for machine translat...\n",
      "3      I don't know the specific transformations use...\n",
      "4      BART performs comparably to other pretraining...\n",
      "5      BART's architecture differs from BERT and GPT...\n",
      "6      The key advantage of BART's setup is that it ...\n",
      "7      The purpose of the in-filling scheme in BART ...\n",
      "8      BART performs well in fine-tuned text generat...\n",
      "9      BART brings a 1.1 BLEU increase over a back-t...\n",
      "10     I don't know the specific pre-training object...\n",
      "11     BART uses token masking in its pre-training o...\n",
      "12     The text does not provide specific informatio...\n",
      "13     The significance of token masking in BART's p...\n",
      "14     BART performs similarly to other models on th...\n",
      "15     The key trends observed in the results of pre...\n",
      "16     The experimental setup for BART's large-scale...\n",
      "17     BART performs significantly better on summari...\n",
      "18     The text does not provide specific results fo...\n",
      "19     The F1 score of BART on the SQuAD 1.1 task is...\n",
      "20     I don't know the accuracy of BART on the MNLI...\n",
      "21     I don't know the specific perplexity (Valid P...\n",
      "22     I don't know the exact ROUGE scores for BART ...\n",
      "23     BART shows an improvement of up to 6 points o...\n",
      "24     BART achieves an EM/F1 score of 86.1/89.2 on ...\n",
      "Name: ResultLLM, dtype: object\n",
      "llama-2-13b-chat.Q4_K_M.gguf4818b21f-bd85-44c0-8c16-23aa47893b8c.csv\n",
      "sentences_list_result :  0       Sure! Here's the answer to your question bas...\n",
      "1       Sure! Based on the context you provided, her...\n",
      "2       Based on the context provided, I can answer ...\n",
      "3       Based on the context provided, I can answer ...\n",
      "4       Based on the given context, I can answer the...\n",
      "5       Sure, I can answer that question based on th...\n",
      "6       Based on the context provided, the key advan...\n",
      "7       Based on the context provided, the purpose o...\n",
      "8       Based on the information provided, BART perf...\n",
      "9       Based on the context provided, here is the a...\n",
      "10      Based on the context provided, I can answer ...\n",
      "11      Sure! Based on the context you provided, her...\n",
      "12      Sure! Based on the context you provided, her...\n",
      "13      Sure, I can answer that question based on th...\n",
      "14      Based on the given context, here is how BART...\n",
      "15      Sure! Here's the answer to your question bas...\n",
      "16      Sure! Based on the context you provided, I c...\n",
      "17      Sure! Based on the provided context, here's ...\n",
      "18      Sure! Based on the information provided, her...\n",
      "19      Sure! Based on the information provided, the...\n",
      "20      Based on the information provided in the tex...\n",
      "21      Sure! Based on the information provided in t...\n",
      "22      Sure! Based on the information provided, I c...\n",
      "23      Sure! Here's the answer to your question bas...\n",
      "24      Sure! Based on the information provided, her...\n",
      "Name: ResultLLM, dtype: object\n",
      "starling-lm-7b-alpha.Q4_K_M.gguf0fdd46ab-267e-453f-833c-9d3df391ce3d.csv\n",
      "sentences_list_result :  0      BART stands for Bidirectional Attention with ...\n",
      "1      Some advantages of BART's setup include consi...\n",
      "2      BART handles fine-tuning for machine translat...\n",
      "3      I don't know the specific transformations use...\n",
      "4      Based on the provided context, BART performs ...\n",
      "5      BART's architecture differs from BERT and GPB...\n",
      "6      The key advantage of BART's setup is that it ...\n",
      "7      The purpose of the in-filling scheme in BAART...\n",
      "8      BART performs well in fine-tuned text generat...\n",
      "9      BAART brings several improvements to machine ...\n",
      "10     I don't know the specific pre-training object...\n",
      "11     BAART uses token masking in its pre-training ...\n",
      "12     The tasks that BAART is evaluated on include ...\n",
      "13     The significance of token masking in BART's p...\n",
      "14     BART performs similarly to other models on th...\n",
      "15     The key trends observed in the results of pre...\n",
      "16     The experimental setup for BART's large-scale...\n",
      "17     BART performs significantly better on summari...\n",
      "18     The results of BAART on dialogue response gen...\n",
      "19     The F1 score of BART on the SQuAD 1.1 task is...\n",
      "20     I don't know the accuracy of BART on the MNLI...\n",
      "21     I don't know the exact perplexity value of BA...\n",
      "22     I don't know the exact ROUGE scores for BART ...\n",
      "23     BART shows an improvement of up to 6 points o...\n",
      "24     BART achieves an EM/F1 score of 86.1/89.2 on ...\n",
      "Name: ResultLLM, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataframe_report = create_df_results(path_results,path_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>method_preprocessing</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>context_length</th>\n",
       "      <th>chunk_technique</th>\n",
       "      <th>temperature</th>\n",
       "      <th>gpu_layers</th>\n",
       "      <th>search_kwargs</th>\n",
       "      <th>size_model</th>\n",
       "      <th>size_answer</th>\n",
       "      <th>reorder</th>\n",
       "      <th>quantization</th>\n",
       "      <th>device</th>\n",
       "      <th>rate</th>\n",
       "      <th>rate_table</th>\n",
       "      <th>rate_text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openchat_3.5.Q4_K_M.gguf</td>\n",
       "      <td>mistral</td>\n",
       "      <td>PaddleOCR</td>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>RecursiveTextSplitter</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7B</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>GGUF</td>\n",
       "      <td>MPS</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>306.62558579444885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openchat_3.5.Q4_K_M.gguf</td>\n",
       "      <td>mistral</td>\n",
       "      <td>PaddleOCR</td>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>RecursiveTextSplitter</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7B</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>GGUF</td>\n",
       "      <td>MPS</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>298.12156772613525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-13b-chat.Q4_K_M.gguf</td>\n",
       "      <td>llama</td>\n",
       "      <td>PaddleOCR</td>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>RecursiveTextSplitter</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7B</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>GGUF</td>\n",
       "      <td>MPS</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>573.254804611206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>starling-lm-7b-alpha.Q4_K_M.gguf</td>\n",
       "      <td>mistral</td>\n",
       "      <td>PaddleOCR</td>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>RecursiveTextSplitter</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7B</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>GGUF</td>\n",
       "      <td>MPS</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>417.3041818141937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model_name model_type method_preprocessing  \\\n",
       "0          openchat_3.5.Q4_K_M.gguf    mistral            PaddleOCR   \n",
       "1          openchat_3.5.Q4_K_M.gguf    mistral            PaddleOCR   \n",
       "2      llama-2-13b-chat.Q4_K_M.gguf      llama            PaddleOCR   \n",
       "3  starling-lm-7b-alpha.Q4_K_M.gguf    mistral            PaddleOCR   \n",
       "\n",
       "          embedding_model chunk_size chunk_overlap context_length  \\\n",
       "0  BAAI/bge-large-en-v1.5        200            64           2048   \n",
       "1  BAAI/bge-large-en-v1.5        200            64           2048   \n",
       "2  BAAI/bge-large-en-v1.5        200            64           2048   \n",
       "3  BAAI/bge-large-en-v1.5        200            64           2048   \n",
       "\n",
       "         chunk_technique temperature gpu_layers search_kwargs size_model  \\\n",
       "0  RecursiveTextSplitter         0.1         20             5         7B   \n",
       "1  RecursiveTextSplitter         0.1         20             5         7B   \n",
       "2  RecursiveTextSplitter         0.1         20             5         7B   \n",
       "3  RecursiveTextSplitter         0.1         20             5         7B   \n",
       "\n",
       "  size_answer reorder quantization device  rate rate_table rate_text  \\\n",
       "0         200    True         GGUF    MPS  0.76   0.166667  0.947368   \n",
       "1         200    True         GGUF    MPS  0.64        0.0  0.842105   \n",
       "2         200    True         GGUF    MPS  0.72   0.166667  0.894737   \n",
       "3         200    True         GGUF    MPS  0.76   0.166667  0.947368   \n",
       "\n",
       "                 time  \n",
       "0  306.62558579444885  \n",
       "1  298.12156772613525  \n",
       "2    573.254804611206  \n",
       "3   417.3041818141937  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
